{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5648adf-2530-4a84-95e2-a26ce2efaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0efd15f-4763-41de-a8a6-446570bf6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba14b2a-6fbc-4944-a1c2-9a7deea8a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 08:13:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/08/27 08:13:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "scala_version = '2.12' \n",
    "spark_version = '3.5.1'\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "            .appName(\"Producer\")\n",
    "            .config(\"spark.jars\", \",\".join([\n",
    "            \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar\",\n",
    "            \"/opt/spark/jars/kafka-clients-3.2.0.jar\",\n",
    "            \"/opt/spark/jars/commons-pool2-2.12.0.jar\",\n",
    "            \"/opt/spark/jars/spark-streaming-kafka-0-10-assembly_2.12-3.5.1.jar\",\n",
    "            \"/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar\"\n",
    "            ]))\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fd2381-93e0-4ce3-8ab1-25f30abd3660",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83432e45-9590-4623-a37f-b7f0af462742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,/opt/spark/jars/kafka-clients-3.2.0.jar,/opt/spark/jars/commons-pool2-2.12.0.jar,/opt/spark/jars/spark-streaming-kafka-0-10-assembly_2.12-3.5.1.jar,/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().get(\"spark.jars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5eee629-5034-43ce-8630-8edcab5067b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Producer():\n",
    "    def __init__(self):\n",
    "        self.BOOTSTRAP_SERVER = \"192.168.1.53:9092\"\n",
    "        self.base_dir = \"/home/iceberg/notebooks\"\n",
    "        self.topic = \"botPredict\"\n",
    "        self.processingTime = \"5 seconds\"\n",
    "\n",
    "    def get_schema(self):\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "        schema = StructType([\n",
    "            StructField(\"ID\", StringType(), True),\n",
    "            StructField(\"BOT_ID\", StringType(), True),\n",
    "            StructField(\"TEXT\", StringType(), True),\n",
    "            StructField(\"INTENT_CONFIDENCE\", StringType(), True),\n",
    "            StructField(\"INTENT_NAME\", StringType(), True),\n",
    "            StructField(\"STEP\", IntegerType(), True),\n",
    "            StructField(\"NLU_THRESHOLD\", StringType(), True),\n",
    "            StructField(\"SENDER_ID\", StringType(), True),\n",
    "            StructField(\"SOURCE\", StringType(), True),\n",
    "            StructField(\"CREATED_TIME\", StringType(), True),\n",
    "            StructField(\"LAST_UPDATED_TIME\", StringType(), True),\n",
    "            StructField(\"ID_CHATLOG\", StringType(), True),\n",
    "            StructField(\"UPDATED_INTENT\", StringType(), True),\n",
    "            StructField(\"LEN_CARD_DATA\", IntegerType(), True),\n",
    "            StructField(\"STATUS_DELETE\", StringType(), True),\n",
    "            StructField(\"STATUS_CONFIRM\", StringType(), True),\n",
    "            StructField(\"INTENT_MAP_CLICK_BUTTON\", StringType(), True)\n",
    "        ])\n",
    "        return schema\n",
    "\n",
    "\n",
    "    def read_chat_data(self, path):\n",
    "        import pyspark.sql.functions as f\n",
    "        predict_df = spark.readStream \\\n",
    "            .format(\"csv\") \\\n",
    "            .schema(self.get_schema()) \\\n",
    "            .option(\"delimiter\", \"|\") \\\n",
    "            .option(\"escape\", \"\\\"\") \\\n",
    "            .option(\"multiline\", \"true\") \\\n",
    "            .load(path)\n",
    "        \n",
    "        fixed_predict_df = predict_df.withColumn(\"CREATED_TIME\", f.to_timestamp(\"CREATED_TIME\", \"dd-MMM-yy hh.mm.ss.SSSSSSSSS a\")) \\\n",
    "                            .withColumn(\"LAST_UPDATED_TIME\", f.to_timestamp(\"LAST_UPDATED_TIME\", \"dd-MMM-yy hh.mm.ss.SSSSSSSSS a\"))\n",
    "\n",
    "        return fixed_predict_df\n",
    "\n",
    "    def get_kafka_message(self, df, key):\n",
    "        return df.selectExpr(f\"{key} as key\", \"to_json(struct(*)) as value\")\n",
    "\n",
    "    # Topic: bot_ddvc_hcm_bot_predict\n",
    "    def send_message(self, kafka_df):\n",
    "        return (kafka_df.writeStream\n",
    "                    .format(\"kafka\")\n",
    "                    .queryName(\"kafka-producer\")\n",
    "                    .option(\"kafka.bootstrap.servers\", self.BOOTSTRAP_SERVER)\n",
    "                    .option(\"topic\", f\"{self.topic}\")\n",
    "                    .option(\"checkpointLocation\", f\"{self.base_dir}/checkpoints/producer\")\n",
    "                    .outputMode(\"append\")\n",
    "                    .trigger(processingTime = f\"{self.processingTime}\")\n",
    "                    .start()\n",
    "               )\n",
    "\n",
    "    def clean(self):\n",
    "        import shutil\n",
    "        shutil.rmtree(\"/home/iceberg/notebooks/checkpoints/producer\")\n",
    "\n",
    "    def process(self):\n",
    "        print(f\"Starting Kafka Producer Stream...\", '')\n",
    "        path = f\"{self.base_dir}/data\"\n",
    "        df = self.read_chat_data(path)\n",
    "        # Chọn cột \"ID\" làm key và toàn bộ cột trong dataframe làm value\n",
    "        kafka_df = self.get_kafka_message(df, \"ID\")\n",
    "        sQuery = self.send_message(kafka_df)\n",
    "        print(\"Done\\n\")\n",
    "        return sQuery  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5304e439-93f1-4eb2-bfa6-d80437cad10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Kafka Producer Stream... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 08:13:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "producer = Producer()\n",
    "producer.clean()\n",
    "sQuery = producer.process()\n",
    "sQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5878154e-25b6-4151-88bd-1689ff2552dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"/home/iceberg/notebooks/checkpoints/producer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e06c73-11db-48a8-aa1e-04b8d57c40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in spark.streams.active:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06467f65-4c5f-4b11-af86-051fcc9ab26a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

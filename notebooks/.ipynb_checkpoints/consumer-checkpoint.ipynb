{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c707742f-7bf9-4094-af84-e2004cc199a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea51b3d2-2e84-4763-b6e9-3bbb0a87dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f708c5db-1c73-4e46-a338-35f13f98e85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 08:14:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "scala_version = '2.12' \n",
    "spark_version = '3.5.1'\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "            .appName(\"Consumer\")\n",
    "            .config(\"spark.jars\", \",\".join([\n",
    "            \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar\",\n",
    "            \"/opt/spark/jars/kafka-clients-3.2.0.jar\",\n",
    "            \"/opt/spark/jars/commons-pool2-2.12.0.jar\",\n",
    "            \"/opt/spark/jars/spark-streaming-kafka-0-10-assembly_2.12-3.5.1.jar\",\n",
    "            \"/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar\"\n",
    "            ]))\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61456d1-31a9-4a41-9882-fbf907b3e258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,/opt/spark/jars/kafka-clients-3.2.0.jar,/opt/spark/jars/commons-pool2-2.12.0.jar,/opt/spark/jars/spark-streaming-kafka-0-10-assembly_2.12-3.5.1.jar,/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().get(\"spark.jars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e0ddde3-513a-4e86-924e-36b3ed51eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Consumer():\n",
    "    def __init__(self):\n",
    "        self.BOOTSTRAP_SERVER = \"192.168.1.53:9092\"\n",
    "        self.base_dir = \"/home/iceberg/notebooks\"\n",
    "        self.topic = \"botPredict\"\n",
    "        self.tableName = \"bot_db.bot_ddvc_hcm_bot_predict\"\n",
    "\n",
    "    def get_schema(self):\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "        schema = StructType([\n",
    "            StructField(\"ID\", StringType(), True),\n",
    "            StructField(\"BOT_ID\", StringType(), True),\n",
    "            StructField(\"TEXT\", StringType(), True),\n",
    "            StructField(\"INTENT_CONFIDENCE\", StringType(), True),\n",
    "            StructField(\"INTENT_NAME\", StringType(), True),\n",
    "            StructField(\"STEP\", IntegerType(), True),\n",
    "            StructField(\"NLU_THRESHOLD\", StringType(), True),\n",
    "            StructField(\"SENDER_ID\", StringType(), True),\n",
    "            StructField(\"SOURCE\", StringType(), True),\n",
    "            StructField(\"CREATED_TIME\", TimestampType(), True),\n",
    "            StructField(\"LAST_UPDATED_TIME\", TimestampType(), True),\n",
    "            StructField(\"ID_CHATLOG\", StringType(), True),\n",
    "            StructField(\"UPDATED_INTENT\", StringType(), True),\n",
    "            StructField(\"LEN_CARD_DATA\", IntegerType(), True),\n",
    "            StructField(\"STATUS_DELETE\", StringType(), True),\n",
    "            StructField(\"STATUS_CONFIRM\", StringType(), True),\n",
    "            StructField(\"INTENT_MAP_CLICK_BUTTON\", StringType(), True)\n",
    "        ])\n",
    "        return schema\n",
    "\n",
    "    def ingest_from_kafka(self):\n",
    "        import pyspark.sql.functions as f\n",
    "        kafka_df = (spark.readStream\n",
    "                        .format(\"kafka\")\n",
    "                        .option(\"kafka.bootstrap.servers\", self.BOOTSTRAP_SERVER)\n",
    "                        .option(\"subscribe\", f\"{self.topic}\")\n",
    "                        .option(\"startingOffsets\", \"earliest\")\n",
    "                        .option(\"maxOffsetsPerTrigger\", 10)\n",
    "                        .load() \n",
    "        )\n",
    "        return kafka_df\n",
    "\n",
    "    def get_kafka_message(self, kafka_df):\n",
    "        from pyspark.sql.functions import from_json, cast, to_timestamp\n",
    "        raw_df = kafka_df.select(\n",
    "            kafka_df.key.cast(\"string\").alias(\"key\"),\n",
    "            from_json(kafka_df.value.cast(\"string\"), self.get_schema()).alias(\"value\"),\n",
    "            \"topic\",\n",
    "            \"timestamp\"            \n",
    "        )\n",
    "        \n",
    "        return raw_df\n",
    "\n",
    "    def get_quality_df(self, raw_df):\n",
    "        from pyspark.sql.functions import from_json, cast, to_timestamp\n",
    "        predict_df = raw_df.select(\"value.*\").filter(raw_df.value.ID != \"ID\")\n",
    "        predict_df = predict_df.withColumn(\"CREATED_TIME\", to_timestamp(\"CREATED_TIME\", \"dd-MMM-yy hh.mm.ss.SSSSSSSSS a\")) \\\n",
    "                            .withColumn(\"LAST_UPDATED_TIME\", to_timestamp(\"LAST_UPDATED_TIME\", \"dd-MMM-yy hh.mm.ss.SSSSSSSSS a\"))\n",
    "\n",
    "        return predict_df\n",
    "        \n",
    "    def clean(self):\n",
    "        import shutil\n",
    "        shutil.rmtree(f\"{self.base_dir}/checkpoints/iceberg-consumer\")\n",
    "\n",
    "    def process(self):\n",
    "        print(\"Start streaming...\", end='')\n",
    "        kafka_df = self.ingest_from_kafka()\n",
    "        raw_df = self.get_kafka_message(kafka_df)\n",
    "        predict_df = self.get_quality_df(raw_df)\n",
    "        # self.clean()\n",
    "        sQuery = (predict_df.writeStream\n",
    "                      .format(\"iceberg\")\n",
    "                      .queryName(\"iceberg-ingestion\")\n",
    "                      .option(\"checkpointLocation\", f\"{self.base_dir}/checkpoints/iceberg-consumer\")\n",
    "                      .outputMode(\"append\")\n",
    "                      .trigger(processingTime = \"5 seconds\")\n",
    "                      .toTable(f\"{self.tableName}\")\n",
    "        )\n",
    "        print(\"Done\")\n",
    "        return sQuery \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0c11dd-1446-4e5c-906a-63beb09b3800",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = Consumer()\n",
    "consumer.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52346688-53c5-4aa9-8e36-76606d3c3371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start streaming...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 08:14:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/08/27 08:14:42 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "consumer = Consumer()\n",
    "sQuery = consumer.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d187dae-ddb0-4645-a169-5e69525c8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in spark.streams.active:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfeba9a1-93e2-401c-84c0-eb4864da1927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "CREATE TABLE IF NOT EXISTS bot_db.bot_ddvc_hcm_bot_predict (\n",
    "    `ID` STRING,\n",
    "    `BOT_ID` STRING,\n",
    "    `TEXT` STRING,\n",
    "    `INTENT_CONFIDENCE` STRING,\n",
    "    `INTENT_NAME` STRING,\n",
    "    `STEP` INT,\n",
    "    `NLU_THRESHOLD` STRING,\n",
    "    `SENDER_ID` STRING,\n",
    "    `SOURCE` STRING,\n",
    "    `CREATED_TIME` TIMESTAMP,\n",
    "    `LAST_UPDATED_TIME` TIMESTAMP,\n",
    "    `ID_CHATLOG` STRING,\n",
    "    `UPDATED_INTENT` STRING,\n",
    "    `LEN_CARD_DATA` INT,\n",
    "    `STATUS_DELETE` STRING,\n",
    "    `STATUS_CONFIRM` STRING,\n",
    "    `INTENT_MAP_CLICK_BUTTON` STRING\n",
    ") USING iceberg\n",
    "PARTITIONED BY (days(`CREATED_TIME`));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14076bf3-28ad-47b7-9473-2a0e783aa721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>count(1)</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>296</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------+\n",
       "| count(1) |\n",
       "+----------+\n",
       "|      296 |\n",
       "+----------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT count(*) FROM bot_db.bot_ddvc_hcm_bot_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47c5220b-dc2e-4ac9-a79e-835906ae959a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>committed_at</th>\n",
       "            <th>snapshot_id</th>\n",
       "            <th>parent_id</th>\n",
       "            <th>operation</th>\n",
       "            <th>manifest_list</th>\n",
       "            <th>summary</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2024-08-27 08:10:28.707000</td>\n",
       "            <td>1440165704216497019</td>\n",
       "            <td>None</td>\n",
       "            <td>append</td>\n",
       "            <td>s3://warehouse/bot_db/bot_ddvc_hcm_bot_predict/metadata/snap-1440165704216497019-1-4357d42f-b151-4fd3-b7e8-642f5791df91.avro</td>\n",
       "            <td>{&#x27;spark.app.id&#x27;: &#x27;local-1724746144551&#x27;, &#x27;changed-partition-count&#x27;: &#x27;0&#x27;, &#x27;total-equality-deletes&#x27;: &#x27;0&#x27;, &#x27;total-position-deletes&#x27;: &#x27;0&#x27;, &#x27;total-delete-files&#x27;: &#x27;0&#x27;, &#x27;spark.sql.streaming.epochId&#x27;: &#x27;0&#x27;, &#x27;total-files-size&#x27;: &#x27;0&#x27;, &#x27;spark.sql.streaming.queryId&#x27;: &#x27;9950349d-e6e0-4541-a862-d86b7ac52e9e&#x27;, &#x27;total-records&#x27;: &#x27;0&#x27;, &#x27;total-data-files&#x27;: &#x27;0&#x27;}</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------------------------+---------------------+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
       "|               committed_at |         snapshot_id | parent_id | operation |                                                                                                                manifest_list |                                                                                                                                                                                                                                                                                                                                             summary |\n",
       "+----------------------------+---------------------+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
       "| 2024-08-27 08:10:28.707000 | 1440165704216497019 |      None |    append | s3://warehouse/bot_db/bot_ddvc_hcm_bot_predict/metadata/snap-1440165704216497019-1-4357d42f-b151-4fd3-b7e8-642f5791df91.avro | {'spark.app.id': 'local-1724746144551', 'changed-partition-count': '0', 'total-equality-deletes': '0', 'total-position-deletes': '0', 'total-delete-files': '0', 'spark.sql.streaming.epochId': '0', 'total-files-size': '0', 'spark.sql.streaming.queryId': '9950349d-e6e0-4541-a862-d86b7ac52e9e', 'total-records': '0', 'total-data-files': '0'} |\n",
       "+----------------------------+---------------------+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM bot_db.bot_ddvc_hcm_bot_predict.snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc834542-8ec1-47a6-b592-313659075809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 08:14:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE bot_db.bot_ddvc_hcm_bot_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6192c-6a75-48dd-b480-570c6d2c8652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
